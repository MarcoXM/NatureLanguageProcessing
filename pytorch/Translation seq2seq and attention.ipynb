{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_token = 0\n",
    "SOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self,name):\n",
    "        \n",
    "        self.name = name\n",
    "        self.word2idx = {}\n",
    "        self.word2count = {}\n",
    "        self.idx2word = {0:'EOS',1:'SOS'}\n",
    "        self.num_words = 2\n",
    "        \n",
    "    def add_sentence(self,sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "            \n",
    "    def add_word(self,word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.idx2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "france = Lang('france')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Lang at 0x10fae1358>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "france\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions!\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lang(path,lang1,lang2,reverse =False):\n",
    "    print('Starting ! ! !')\n",
    "    lines = open(path +'%s-%s.txt' % (lang1, lang2), encoding='utf-8').read().strip().split('\\n')\n",
    "    \n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    \n",
    "     # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodeRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size,hidden_size):\n",
    "        super(EncodeRNN,self).__init__()\n",
    "        self.hidden=hidden_size\n",
    "        \n",
    "        self.emb = nn.Embedding(input_size,hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size,hidden_size)\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        embed = self.emb(x).view(1,1,-1)\n",
    "        output,hidden = self.gru(embed,hidden)\n",
    "        return output,hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden, device=device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecodeRNN(nn.Module):\n",
    "    def __init__(self,hidden,output):\n",
    "        super(DecodeRNN,self).__init__()\n",
    "        self.hidden = hidden\n",
    "        \n",
    "        self.emb = nn.Embedding(output,hidden) # word2vec dimension == hidden !\n",
    "        self.gru = nn.GRU(hidden,hidden) # word to word ,same size\n",
    "        self.fc = nn.Linear(hidden,output) # pick word from vocab!\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        \n",
    "        embed = self.emb(x).view(1,1,-1)\n",
    "        output,hidden = nn.gru(embed,hidden)\n",
    "        output = F.log_softmax(output,dim=1)\n",
    "        return output,hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10 # 10 words at large !\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(pair):\n",
    "    return len(pair[0].split(' ')) < MAX_LENGTH and len(pair[1].split(' ')) < MAX_LENGTH and pair[0].startswith(eng_prefixes)\n",
    "    #getting those pairs len(Eng) <10 and start with...\n",
    "    \n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = read_lang('data/',lang1, lang2)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        \n",
    "        input_lang.add_sentence(pair[0])\n",
    "        output_lang.add_sentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.num_words)\n",
    "    print(output_lang.name, output_lang.num_words)\n",
    "    return input_lang, output_lang, pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ! ! !\n",
      "Read 170190 sentence pairs\n",
      "Trimmed to 12702 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 3050\n",
      "fra 4736\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData(lang1='eng', lang2='fra')\n",
    "#print(random.choice(pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i m .', 'j ai ans .']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[0] # One pair example!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filterPair(pairs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to do \n",
    "# we have raw token data ! words !\n",
    "# now the workflow is that turn words into int, then we can turn int into our embedding(one hot vector)\n",
    "\n",
    "def sentence2idx(lang,sentence):\n",
    "    return [lang.word2idx[word] for word in sentence.split(' ')]\n",
    "\n",
    "def sentenc2tensor(lang,sentence):\n",
    "    idx = sentence2idx(lang,sentence)\n",
    "    idx.append(EOS_token) ##  Adding the ending word!\n",
    "    return torch.LongTensor(idx).view(-1, 1).to(DEVICE)\n",
    "\n",
    "def pair2tensor(lang1,lang2,pair):\n",
    "    \n",
    "    x_tensor = sentenc2tensor(lang1,pair[0]) # input language >>>>>> index\n",
    "    y_tensor = sentenc2tensor(lang2,pair[1]) # output language >>>>>>> index\n",
    "    \n",
    "    return x_tensor,y_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoder(nn.Module): # Multiplicative Attention\n",
    "    def __init__(self,hidden_size,output_size,dropout_rate = 0.2):\n",
    "        super(AttnDecoder,self).__init__()\n",
    "        \n",
    "        self.hidden = hidden_size # this is word vecter dimension\n",
    "        self.output = output_size # this is vocab size\n",
    "        self.drop = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        \n",
    "        self.emb = nn.Embedding(output_size,hidden_size)\n",
    "        \n",
    "        self.rnn = nn.GRU(hidden_size,hidden_size) # <EOS> inital and as the first word vector!\n",
    "        self.h2newoutput = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.toword = nn.Linear(hidden_size,output_size)\n",
    "        \n",
    "    def forward(self,x,hidden,encoder_matrix):\n",
    "        \n",
    "        emb_vecter = self.emb(x)\n",
    "        #print(emb_vecter.size())\n",
    "        score = F.softmax(torch.matmul(emb_vecter,torch.t(encoder_matrix)),dim=-1)\n",
    "        #print(score)\n",
    "        new_context = torch.matmul(score,encoder_matrix)\n",
    "        #print(new_context.size())\n",
    "        output,hidden = self.rnn(emb_vecter,hidden)\n",
    "        hidden = torch.cat([new_context,output],dim = 2)\n",
    "        #print(new_input.size())\n",
    "        output = F.tanh(self.h2newoutput(hidden))\n",
    "        output = self.drop(output)\n",
    "        final_output = F.log_softmax(self.toword(output),dim = 2)\n",
    "        return final_output,hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden, device=device)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2],\n",
       "        [ 3],\n",
       "        [11],\n",
       "        [ 4],\n",
       "        [ 0]])"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = pair2tensor(input_lang,output_lang,pairs[9])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = EncodeRNN(input_lang.num_words,50)\n",
    "d = AttnDecoder(50,output_lang.num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_hidden = e.initHidden()\n",
    "encoder_matrix = torch.zeros(x.size(0),50)\n",
    "for i in range(x.size(0)):\n",
    "    out,e_hidden = e(x[i],e_hidden)\n",
    "    encoder_matrix[i] = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.6333e-01,  2.0939e-01,  1.9787e-01,  3.5635e-01, -3.5455e-02,\n",
      "         -2.7447e-01, -3.8042e-01, -3.0422e-01,  1.2252e-01, -2.1466e-01,\n",
      "         -2.0349e-01, -9.8748e-02,  4.0235e-02, -3.6948e-01, -9.9609e-02,\n",
      "          3.4834e-01,  9.1425e-02, -9.9497e-02,  2.1240e-01,  3.0851e-01,\n",
      "          2.1479e-01,  2.9206e-01,  1.5038e-01,  2.9462e-01,  2.8715e-02,\n",
      "         -8.6148e-02,  2.0119e-01, -2.1509e-01,  1.5292e-01,  1.1958e-01,\n",
      "         -1.2617e-01,  1.3298e-01,  9.1545e-02, -1.5589e-01,  6.1138e-02,\n",
      "          2.5503e-01,  1.2153e-01, -3.0593e-03,  1.6935e-01, -1.2639e-01,\n",
      "         -5.7941e-02,  1.3696e-01,  3.1911e-02, -4.2443e-02,  5.9668e-02,\n",
      "          4.7592e-01, -2.8284e-01, -1.5854e-02, -1.4259e-01,  1.9773e-01],\n",
      "        [ 7.1972e-02,  7.5361e-02,  2.9119e-01, -5.7573e-02, -3.4091e-02,\n",
      "         -1.1899e-01, -2.2229e-01, -2.4887e-01,  5.9378e-01, -3.3829e-01,\n",
      "         -4.4178e-01, -9.9675e-02, -3.3254e-01, -5.9565e-02, -2.7380e-01,\n",
      "          3.2901e-01, -1.5358e-01, -3.7331e-02,  8.7457e-02, -2.7339e-01,\n",
      "          4.8054e-01, -8.9507e-02, -1.2907e-01,  3.1551e-01, -1.0474e-01,\n",
      "          1.5120e-01,  2.6486e-01, -1.8781e-01,  3.3813e-01, -7.5033e-02,\n",
      "          8.7835e-02,  2.3932e-01, -1.0230e-01, -1.5194e-01, -4.2642e-02,\n",
      "         -1.0574e-01, -1.7871e-01, -5.9094e-02, -2.1435e-01, -2.4697e-01,\n",
      "         -3.6915e-01, -8.9486e-02, -2.6565e-02, -4.9000e-01,  3.2691e-01,\n",
      "          3.8212e-01, -3.2807e-01, -1.6055e-02,  1.1868e-01, -1.0401e-01],\n",
      "        [-4.9912e-02, -1.3322e-01,  1.9761e-01, -1.7043e-01, -1.2077e-01,\n",
      "          3.2748e-02, -1.2818e-01,  2.1916e-01,  5.0799e-01,  2.4750e-01,\n",
      "         -5.3528e-01,  6.8566e-02,  4.1842e-05,  2.6029e-01, -3.3308e-01,\n",
      "         -4.4444e-02, -5.3027e-01, -3.9812e-01,  3.3877e-01, -1.1714e-01,\n",
      "          2.5427e-01, -1.4740e-01, -2.4319e-01,  2.6618e-01,  3.1809e-01,\n",
      "          5.0661e-02,  3.1342e-01, -4.4353e-01,  3.4031e-01,  6.7861e-01,\n",
      "         -4.3825e-01, -5.7539e-02, -1.0655e-01, -1.7087e-01, -2.1347e-01,\n",
      "         -2.5537e-01, -2.0289e-02,  1.4141e-02, -1.6899e-01,  4.3629e-01,\n",
      "         -9.8818e-03,  2.1290e-01, -2.1187e-01,  3.7201e-02,  8.6251e-02,\n",
      "          2.7271e-01, -1.9772e-01, -2.0501e-01,  2.8750e-01,  4.4463e-02],\n",
      "        [-1.7911e-01, -1.7475e-01,  8.0492e-03, -7.7521e-02,  1.1397e-01,\n",
      "         -1.4697e-01, -2.0091e-01,  3.4601e-03,  3.5636e-01,  4.1840e-01,\n",
      "         -2.2775e-01,  3.4315e-01, -2.9149e-02,  2.5138e-01,  1.0702e-02,\n",
      "         -3.5630e-01, -3.5556e-01, -4.0568e-01,  1.4234e-01,  7.8382e-02,\n",
      "         -3.4079e-02,  2.5972e-01,  3.2420e-01,  3.5649e-01,  1.4328e-01,\n",
      "          3.5815e-01,  2.1618e-02, -2.3579e-01,  1.4318e-01,  4.0863e-01,\n",
      "          2.9725e-02,  1.2703e-01, -2.5502e-02, -1.4972e-01, -1.1719e-01,\n",
      "         -2.9107e-01,  3.2384e-02, -1.1412e-01,  5.7097e-03,  2.5318e-01,\n",
      "          3.7561e-01, -1.5946e-01, -4.2065e-01,  5.2094e-02, -1.2380e-01,\n",
      "          1.5518e-01, -3.9883e-01, -4.2205e-01,  3.7060e-01,  4.0722e-02],\n",
      "        [-1.1716e-01, -1.3755e-01, -1.6596e-01, -1.3473e-01,  7.3144e-02,\n",
      "          1.1469e-01, -2.4421e-01, -2.2730e-01,  1.3756e-01,  4.6040e-01,\n",
      "         -3.6315e-01,  4.9772e-01, -5.5037e-01, -4.0587e-01,  1.3118e-01,\n",
      "         -2.6386e-02, -1.4202e-01, -4.4532e-01,  3.6996e-02,  6.2297e-02,\n",
      "          2.8758e-01, -1.7051e-01,  3.1953e-01,  2.6643e-01, -1.5973e-01,\n",
      "          4.6610e-01,  3.3420e-02, -2.4323e-01,  3.5832e-01, -6.2439e-02,\n",
      "         -4.2476e-01,  1.1435e-01, -6.4303e-02, -4.0841e-01,  2.6314e-02,\n",
      "         -1.8419e-01,  1.9039e-01, -4.7946e-02, -2.0268e-02,  1.7561e-01,\n",
      "         -1.3805e-01,  2.3856e-01, -1.0378e-01, -3.9155e-02, -2.5097e-01,\n",
      "          5.3309e-01, -6.6506e-01, -4.3706e-01,  3.8576e-01, -3.2105e-01]],\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_hidden = d.initHidden()\n",
    "decoder_input = torch.tensor([[SOS_token]], device=DEVICE)\n",
    "out,hidden = d(decoder_input,d_hidden,encoder_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 100])"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([[-8.0691]], grad_fn=<MaxBackward0>),\n",
       "indices=tensor([[3919]]))"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(out,dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(input_length, encoder.hidden, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "            loss += criterion(decoder_output.squeeze().view(1,-1), target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output.squeeze().view(1,-1), target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [pair2tensor(input_lang,output_lang,random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input must have 3 dimensions, got 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-371-45ed540d3505>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mattn_decoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttnDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m75000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-369-b9bd992d53c6>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         loss = train(input_tensor, target_tensor, encoder,\n\u001b[0;32m---> 19\u001b[0;31m                      decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-368-2164e773803d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Teacher forcing: Feed the target as the next input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deep-learning/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-360-40eac6bb2b4e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden, encoder_matrix)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mnew_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#print(new_context.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_vecter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m#print(new_input.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deep-learning/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deep-learning/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0m_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_rnn_impls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deep-learning/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deep-learning/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    147\u001b[0m             raise RuntimeError(\n\u001b[1;32m    148\u001b[0m                 'input must have {} dimensions, got {}'.format(\n\u001b[0;32m--> 149\u001b[0;31m                     expected_input_dim, input.dim()))\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             raise RuntimeError(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input must have 3 dimensions, got 2"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncodeRNN(input_lang.num_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoder(hidden_size, output_lang.num_words, dropout_rate=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 300])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = torch.randint(0,5,(10,300)).float()\n",
    "d.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 300])"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.randint(0,5,(1,1,300)).float()\n",
    "c.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000e+00, 1.0089e-43, 1.5629e-18, 9.6025e-24, 9.9998e-01,\n",
       "          0.0000e+00, 0.0000e+00, 4.9060e-35, 1.6701e-05, 0.0000e+00]]])"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = F.softmax(torch.matmul(c, torch.t(d)),dim=2)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.9205e-23, 3.0000e+00, 3.0000e+00, 3.0000e+00, 3.0000e+00,\n",
       "          3.0000e+00, 2.0000e+00, 3.0000e+00, 3.9999e+00, 1.0000e+00,\n",
       "          9.9998e-01, 1.0000e+00, 1.0000e+00, 3.1257e-18, 1.6701e-05,\n",
       "          2.0000e+00, 3.0000e+00, 3.0000e+00, 2.0000e+00, 2.0000e+00,\n",
       "          3.9999e+00, 4.0000e+00, 2.0000e+00, 6.2514e-18, 2.0000e+00,\n",
       "          4.0000e+00, 9.9998e-01, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
       "          3.0000e+00, 1.0000e+00, 3.0000e+00, 3.9999e+00, 4.0000e+00,\n",
       "          1.0000e+00, 1.0000e+00, 2.0000e+00, 2.9999e+00, 3.0000e+00,\n",
       "          3.0000e+00, 6.6806e-05, 4.0000e+00, 3.0000e+00, 9.9998e-01,\n",
       "          3.0000e+00, 3.9999e+00, 3.0000e+00, 2.0000e+00, 4.0000e+00,\n",
       "          3.0000e+00, 1.0000e+00, 2.0000e+00, 6.2514e-18, 1.0001e+00,\n",
       "          1.0000e+00, 3.0000e+00, 2.0000e+00, 1.0001e+00, 2.0000e+00,\n",
       "          1.0001e+00, 1.0000e+00, 9.9998e-01, 2.9999e+00, 3.0000e+00,\n",
       "          3.1257e-18, 2.9999e+00, 1.0000e+00, 2.0000e+00, 3.9999e+00,\n",
       "          3.0000e+00, 3.3403e-05, 1.0000e+00, 3.3403e-05, 4.0000e+00,\n",
       "          4.0000e+00, 2.0000e+00, 3.0000e+00, 1.0000e+00, 2.9999e+00,\n",
       "          1.0001e+00, 3.0000e+00, 9.9998e-01, 2.9999e+00, 2.0000e+00,\n",
       "          3.3403e-05, 3.0000e+00, 9.9998e-01, 2.0000e+00, 4.0000e+00,\n",
       "          3.3403e-05, 2.0000e+00, 3.0000e+00, 4.0000e+00, 3.0000e+00,\n",
       "          4.0000e+00, 4.0000e+00, 4.0000e+00, 2.0000e+00, 2.0000e+00,\n",
       "          6.6806e-05, 1.0001e+00, 4.0000e+00, 4.0000e+00, 2.0000e+00,\n",
       "          3.0000e+00, 1.0000e+00, 4.0000e+00, 1.0000e+00, 3.0000e+00,\n",
       "          3.9999e+00, 1.0000e+00, 1.0001e+00, 2.0000e+00, 2.0000e+00,\n",
       "          3.1257e-18, 3.9999e+00, 4.0000e+00, 4.0000e+00, 1.0001e+00,\n",
       "          2.9999e+00, 3.0000e+00, 3.0000e+00, 1.6701e-05, 2.0000e+00,\n",
       "          6.6806e-05, 3.9999e+00, 3.0000e+00, 4.0000e+00, 2.9999e+00,\n",
       "          2.0000e+00, 1.0000e+00, 1.0000e+00, 3.9999e+00, 1.0001e+00,\n",
       "          2.9999e+00, 1.0001e+00, 2.0000e+00, 4.0000e+00, 3.0000e+00,\n",
       "          3.0000e+00, 1.0001e+00, 3.9999e+00, 3.0000e+00, 3.3403e-05,\n",
       "          1.0001e+00, 6.6806e-05, 1.6701e-05, 1.0000e+00, 2.0000e+00,\n",
       "          2.0000e+00, 2.0000e+00, 2.0000e+00, 1.0000e+00, 1.6701e-05,\n",
       "          3.0000e+00, 1.0000e+00, 2.0000e+00, 4.0000e+00, 1.0000e+00,\n",
       "          4.0000e+00, 4.0000e+00, 3.0000e+00, 5.0104e-05, 6.6806e-05,\n",
       "          2.0000e+00, 4.0000e+00, 4.0000e+00, 6.2514e-18, 4.0000e+00,\n",
       "          1.0001e+00, 2.0000e+00, 1.5629e-18, 1.0000e+00, 4.0000e+00,\n",
       "          3.0000e+00, 3.0000e+00, 3.0000e+00, 4.0000e+00, 4.0000e+00,\n",
       "          2.0000e+00, 3.9999e+00, 1.6701e-05, 2.0000e+00, 6.6806e-05,\n",
       "          6.6806e-05, 2.0000e+00, 1.0001e+00, 1.0000e+00, 6.6806e-05,\n",
       "          1.0000e+00, 1.6701e-05, 4.0000e+00, 1.0000e+00, 3.0000e+00,\n",
       "          9.9998e-01, 2.0000e+00, 4.0000e+00, 2.9999e+00, 4.0000e+00,\n",
       "          9.9998e-01, 3.3403e-05, 1.6701e-05, 1.6701e-05, 4.0000e+00,\n",
       "          6.6806e-05, 2.9999e+00, 3.0000e+00, 9.9998e-01, 4.0000e+00,\n",
       "          1.0001e+00, 2.9999e+00, 4.0000e+00, 1.6701e-05, 2.0000e+00,\n",
       "          3.9999e+00, 1.0000e+00, 3.0000e+00, 2.0000e+00, 2.0000e+00,\n",
       "          4.0000e+00, 4.0000e+00, 3.0000e+00, 3.9999e+00, 6.6806e-05,\n",
       "          3.3403e-05, 2.0000e+00, 3.9999e+00, 4.0000e+00, 3.9999e+00,\n",
       "          3.3403e-05, 3.0000e+00, 4.0000e+00, 2.0000e+00, 1.0000e+00,\n",
       "          3.0000e+00, 2.0000e+00, 6.6806e-05, 3.3403e-05, 3.0000e+00,\n",
       "          3.0000e+00, 2.0000e+00, 3.9999e+00, 2.0000e+00, 9.9998e-01,\n",
       "          2.0000e+00, 3.0000e+00, 4.0000e+00, 2.0000e+00, 9.9998e-01,\n",
       "          5.0104e-05, 3.9999e+00, 3.9999e+00, 1.0000e+00, 1.0000e+00,\n",
       "          6.6806e-05, 1.0000e+00, 3.0000e+00, 1.0001e+00, 5.0104e-05,\n",
       "          3.9999e+00, 4.6886e-18, 2.0000e+00, 1.0001e+00, 3.0000e+00,\n",
       "          2.0000e+00, 6.6806e-05, 5.0104e-05, 2.0000e+00, 1.6701e-05,\n",
       "          3.0000e+00, 1.0001e+00, 3.9999e+00, 2.0000e+00, 5.0104e-05,\n",
       "          4.6886e-18, 2.0000e+00, 3.0000e+00, 3.9999e+00, 6.6806e-05,\n",
       "          6.6806e-05, 4.0000e+00, 1.0000e+00, 2.0000e+00, 2.0000e+00,\n",
       "          2.0000e+00, 3.0000e+00, 2.0000e+00, 1.0001e+00, 3.0000e+00,\n",
       "          3.1258e-18, 3.0000e+00, 3.9999e+00, 3.9999e+00, 2.0000e+00,\n",
       "          1.0000e+00, 1.5629e-18, 1.0000e+00, 6.6806e-05, 6.6806e-05]]])"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = torch.matmul(s,d)\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NLLLoss' object has no attribute 'file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-234-d7e1a519962b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/deep-learning/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    537\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 539\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NLLLoss' object has no attribute 'file'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
