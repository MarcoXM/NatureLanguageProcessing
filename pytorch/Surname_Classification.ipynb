{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self,surname_df,surname_voectorizer):\n",
    "        self.df = surname_df\n",
    "        self.vectorizer = surname_voectorizer\n",
    "        \n",
    "        self.train_df = self.df[self.df.split == 'train']\n",
    "        self.train_size = len(self.tarin_df)\n",
    "        \n",
    "        self.valid_df = self.df[self.df.split == 'valid']\n",
    "        self.valid_size = len(self.valid_df)\n",
    "        \n",
    "        self.test_df = self.df[self.df.split == 'test']\n",
    "        self.test_size = len(self.test_df)\n",
    "        \n",
    "        self.lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'valid': (self.valid_df, self.valid_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "        \n",
    "    @classmethod    \n",
    "    def load_data_build_vectorizer(cls,surname_path,vectorizer_path):\n",
    "        surname_df = pd.read_csv(surname_path)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_path)\n",
    "        return cls(surname_df, vectorizer)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_path):\n",
    "        with open(vectorizer_path) as fp:\n",
    "            return SurnameVectorizer.from_serializable(json.load(fp))\n",
    "        \n",
    "    def save_vectorizer(self, vectorizer_path):\n",
    "   \n",
    "        with open(vectorizer_path, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "            \n",
    "        return self.vectorizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self.lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        surname_vector = self.vectorizer.vectorize(row.names)\n",
    "        nationality_index = self.vectorizer.nationality_vocab.lookup_token(row.namescoutry)\n",
    "        return {'x_surname': surname_vector,\n",
    "                'y_nationality': nationality_index}\n",
    "    \n",
    "    def get_num_batches(self, batch_size):\n",
    "\n",
    "        return len(self) // batch_size\n",
    "# For now, we have finized the Dataset Class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# using the dataloader to get the batch data.\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apparently this is the most important one.\n",
    "class Vocabulary(object): # vocabulary should include all data we have both testing and training\n",
    "    def __init__(self,token2idx = None, add_UNK = True, UNK = '<UNK>'):\n",
    "        \n",
    "        if token2idx == None:\n",
    "            token2idx = {}\n",
    "        \n",
    "        self.token2idx = token2idx\n",
    "        self.idx2token = {v:k for k,v in self.token2idx.items()} #initial two dicts\n",
    "        \n",
    "        self.add_UNK = add_UNK\n",
    "        self.UNK = UNK\n",
    "        \n",
    "        self.UNK_index = -1 # the 0 based in python\n",
    "        \n",
    "        if add_UNK:\n",
    "            self.UNK_index = self.add_token(UNK)\n",
    "        \n",
    "    def serialized(self):\n",
    "        return {'token2idx' : self.token2idx,\n",
    "               'add_UNK': self.add_UNK,\n",
    "               'UNK':self.UNK}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serialied(cls,content):\n",
    "        return cls(**content)\n",
    "    \n",
    "    def add_token(self,token):\n",
    "        \n",
    "        try:\n",
    "            idx = self.token2idx[token]\n",
    "        except KeyError:\n",
    "            idx = len(self.idx2token)\n",
    "            self.token2idx[token] = idx\n",
    "            self.idx2token[idx] = token\n",
    "            \n",
    "    def add_manytoken(self,tokens):\n",
    "        \n",
    "        return [self.add_token(t) for t in tokens]\n",
    "    \n",
    "    def look_up_token(self,token):\n",
    "        \n",
    "        if self.UNK_index >= 0: # if is a UNKNOW word\n",
    "            return self.token2idx.get(token,self.UNK)\n",
    "        else: # We dont have unknow word, so that we directly get it from dict.\n",
    "            return self.token2idx[token]\n",
    "            \n",
    "    def look_up_index(self,idx):\n",
    "        if idx not in self.idx2token:\n",
    "            raise KeyError('There is no such (%d) in the Vocabulary % idx')\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.token2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameVectorizer(object):\n",
    "    def __init__(self,names_vocab,namescountry_vocab):\n",
    "        self.namesvocab = names\n",
    "        self.countryvocab = namescountry\n",
    "    \n",
    "    def vetorize(self,name):\n",
    "        vocab = self.namesvocab\n",
    "        one_hot = np.zeros(len(vocab), dtype=np.float32) # 1 D vector\n",
    "        for token in surname:\n",
    "            one_hot[vocab.lookup_token(token)] = 1\n",
    "\n",
    "        return one_hot\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, surname_df): # getting the vocab\n",
    "        \n",
    "        surname_vocab = Vocabulary(UNK=\"@\")\n",
    "        nationality_vocab = Vocabulary(add_UNK=False)\n",
    "\n",
    "        for index, row in surname_df.iterrows():\n",
    "            for letter in row.surname:\n",
    "                surname_vocab.add_token(letter)\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "\n",
    "        return cls(surname_vocab, nationality_vocab)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        surname_vocab = Vocabulary.from_serialied(contents['surname_vocab'])\n",
    "        nationality_vocab =  Vocabulary.from_serialied(contents['nationality_vocab'])\n",
    "        return cls(surname_vocab=surname_vocab, nationality_vocab=nationality_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'surname_vocab': self.namesvocab.serialized(),\n",
    "                'nationality_vocab': self.countryvocab.serialized()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super(SurnameClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, apply_softmax=False):\n",
    "        intermediate_vector = F.tanh(self.fc1(x))\n",
    "        prediction_vector = self.fc2(intermediate_vector)\n",
    "\n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "\n",
    "        return prediction_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args,model,train_state):\n",
    "    if train_state['index'] == 0:\n",
    "        torch.save(model.state_dict(),train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "        \n",
    "    elif train_state['index'] >= 1:\n",
    "        val_loss_t_1,val_loss_t = train_state['val_loss'][-2:]\n",
    "        \n",
    "        if val_loss_t > train_state['early_stopping_best_val']:\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        \n",
    "        else:\n",
    "            torch.save(model.state_dict(), train_state['model_filename'])\n",
    "            train_state['early_stopping_step'] = 0\n",
    "        \n",
    "        train_state['stop_early'] = train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "        \n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Natural language processing with pytorch is not worthy to buy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
