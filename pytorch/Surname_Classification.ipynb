{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self,surname_df,surname_voectorizer):\n",
    "        self.df = surname_df\n",
    "        self.vectorizer = surname_voectorizer\n",
    "        \n",
    "        self.train_df = self.df[self.df.split == 'train']\n",
    "        self.train_size = len(self.tarin_df)\n",
    "        \n",
    "        self.valid_df = self.df[self.df.split == 'valid']\n",
    "        self.valid_size = len(self.valid_df)\n",
    "        \n",
    "        self.test_df = self.df[self.df.split == 'test']\n",
    "        self.test_size = len(self.test_df)\n",
    "        \n",
    "        self.lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'valid': (self.valid_df, self.valid_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "        \n",
    "    @classmethod    \n",
    "    def load_data_build_vectorizer(cls,surname_path,vectorizer_path):\n",
    "        surname_df = pd.read_csv(surname_path)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_path)\n",
    "        return cls(surname_df, vectorizer)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_path):\n",
    "        with open(vectorizer_path) as fp:\n",
    "            return SurnameVectorizer.from_serializable(json.load(fp))\n",
    "        \n",
    "    def save_vectorizer(self, vectorizer_path):\n",
    "   \n",
    "        with open(vectorizer_path, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "            \n",
    "        return self.vectorizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self.lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        surname_vector = self.vectorizer.vectorize(row.names)\n",
    "        nationality_index = self.vectorizer.nationality_vocab.lookup_token(row.namescoutry)\n",
    "        return {'x_surname': surname_vector,\n",
    "                'y_nationality': nationality_index}\n",
    "    \n",
    "    def get_num_batches(self, batch_size):\n",
    "\n",
    "        return len(self) // batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object): # vocabulary should include all data we have both testing and training\n",
    "    def __init__(self,token2idx = None, add_UNK = True, UNK = '<UNK>'):\n",
    "        \n",
    "        if token2idx == None:\n",
    "            token2idx = {}\n",
    "        \n",
    "        self.token2idx = token2idx\n",
    "        self.idx2token = {v,k for k,v in self.token2idx.items()} #initial two dicts\n",
    "        \n",
    "        self.add_UNK = add_UNK\n",
    "        self.UNK = UNK\n",
    "        \n",
    "        self.UNK_index = -1 # the 0 based in python\n",
    "        \n",
    "        if add_UNK:\n",
    "            self.UNK_index = self.add_token(UNK)\n",
    "        \n",
    "    def serialized(self):\n",
    "        return {'token2idx' : self.token2idx,\n",
    "               'add_UNK': self.add_UNK,\n",
    "               'UNK':self.UNK}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serialied(cls,content):\n",
    "        return cls(**content)\n",
    "    \n",
    "    def add_token(self,token):\n",
    "        \n",
    "        try:\n",
    "            idx = self.token2idx[token]\n",
    "        except KeyError:\n",
    "            idx = len(self.idx2token)\n",
    "            self.token2idx[token] = idx\n",
    "            self.idx2token[idx] = token\n",
    "            \n",
    "    def add_manytoken(self,tokens):\n",
    "        \n",
    "        return [self.add_token(t) for t in tokens]\n",
    "    \n",
    "    def look_up_token(self,token):\n",
    "        \n",
    "        if self.UNK_index >= 0:\n",
    "            return self.token2idx.get(token,self.UNK)\n",
    "        else: # We dont have unknow word, so that we directly get it from dict.\n",
    "            return self.token2idx[token]\n",
    "            \n",
    "            \n",
    "    \n",
    "        \n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
