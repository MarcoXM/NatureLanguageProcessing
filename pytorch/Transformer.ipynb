{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cec06c5bbfaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math,copy,time\n",
    "from torch.autograd import Variable\n",
    "%matplotlib inline \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    '''\n",
    "    High Level structure of the NMT model\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,encoder, # input word to vector\n",
    "                decoder, # hidden  to vector\n",
    "                x_emb, # input representation layers\n",
    "                y_emb, # output representation\n",
    "                clf): # final classifier , pick the right word.\n",
    "        super(EncoderDecoder,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.x_emb = x_emb\n",
    "        self.y_emb = y_emb\n",
    "        self.clf = clf\n",
    "        \n",
    "    \n",
    "    def forward(self, x,\n",
    "               y,\n",
    "               x_mask,\n",
    "               y_mask):\n",
    "        h = self.encoder(x,x_mask)\n",
    "        y = self.decoder(h,x_mask, # attention \n",
    "                        y,y_mask)\n",
    "        return y\n",
    "    \n",
    "    def encoder(self,x,x_mask):\n",
    "        return self.encoder(self.x_emb(x),x_mask) # getting the hidden state matrix\n",
    "    \n",
    "    def decoder(self,hidden_matrix,x_mask,y,y_mask):\n",
    "        return self.decoder(self.y_emb(y),hidden_matrix,#dot_product\n",
    "                           x_mask,y_mask)\n",
    "    \n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self,dimension,vocab):\n",
    "        super(Classifier,self).__init__()\n",
    "        self.fc = nn.Linear(dimension,vocab)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return F.log_softmax(self.fc(x),dim = -1) # So the loss would be NNloss\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module,N):\n",
    "    return nn.ModuleList(*[copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,layer, N):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.layers = clone(layer,N)\n",
    "        self.norm = LayerNorm(layer.size) # later when you initialize layer\n",
    "       \n",
    "        \n",
    "    def forward(self,x,x_mask):\n",
    "        for lyer in self.layers:\n",
    "            x = lyer(x,x_mask)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        return x     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layernorm(nn.Module): # Layer norm is not batch norm ! #it norm for features in the instance\n",
    "    def __init__(self,features, eps = 1e-6):\n",
    "        super(Layernorm,self).__init__()\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self,x):\n",
    "        mean = torch.mean(x,dim = -1)\n",
    "        std = torch.std(x,dim = -1)\n",
    "        return (x - mean)/(std + self.eps)**0.5 # Based on the original paper\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = Layernorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x))) # residual\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,size,self_attn,fc,dropout):\n",
    "        super(EncoderLayer).__init__()\n",
    "        self.attn = self_attn\n",
    "        self.fc = fc\n",
    "        self.sublayer = clones(SublayerConnection(size,dropout),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,layers,N):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.layers = clones(layers,N)\n",
    "        self.norm = Layernorm(layer.size)\n",
    "        \n",
    "    def forward(self,x,hidden,x_mask,y_mask):\n",
    "        for lyer in self.layers:\n",
    "            x = layer(x,hidden,x_mask,y_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,size, self_attn, x_attn, fc, dropout):\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        self.size = size\n",
    "        self.attn = self_attn\n",
    "        self.x_attn = x_attn\n",
    "        self.fc = fc\n",
    "        self.sublayers = clones(SublayerConnection(size,dropout),3)\n",
    "        \n",
    "    def forward(self,x,hidden ,x_mask,y_mask):\n",
    "        x = self.sublayers[0](x,lambda x: self.attn(x,x,x,y_mask))\n",
    "        x = slef.sublayers[1](x, lambda x: self.x_attn(x,m,m,x_mask))\n",
    "        return self.sublayers[2](x,self.fc)\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sebsequent(size):\n",
    "    attn_shape = (1,size,size)\n",
    "    subseq_mask = np.triu(np.ones(attn_shape),k = 1).astype('uint8')\n",
    "    return torch.from_numpy(subseq_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(sebsequent(20)[0])\n",
    "sebsequent(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Attn(k,q,v,mask = None, dropout = None):\n",
    "    d_k = q.size(-1)\n",
    "    scores = torch.matmul(q,k.transpose(-2,-1))/math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask ==0, -1e9)\n",
    "    weight = F.softmax(scores,dim = -1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        weight = dropout(weight)\n",
    "        \n",
    "    return torch.matmul(weight,v),weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAttn(nn.Module):\n",
    "    def __init__(self,h,d_model,dropout = 0.1):\n",
    "        super(MultiAttn,self).__init__()\n",
    "        assert d_model%h == 0\n",
    "        self.d_k = d_model//h\n",
    "        self.h = h\n",
    "        self.fcs = clone(nn.Linear(d_model,d_model),4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,q,k,v,mask = None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1) # add dimension\n",
    "        batch_size = q.size(0)\n",
    "        factor = [l(x).view(batch_size,-1,self.h,self.d_k) for l,x in zip(self.fcs,q,k,v)]\n",
    "            \n",
    "        x,self.attn = Attn(*factor,mask=mask, dropout=self.dropout)\n",
    "        \n",
    "        x = x.transpose(1,2).contiguous().view(batch_size,-1,self.h*self.d_k)\n",
    "        return self.fcs[-1](x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionFC(nn.Module):\n",
    "    def __init__(self,d_model,d_ff,dropout = 0.1):\n",
    "        super(PositionFC,self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model,d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff,d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.w_1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.w_2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Embeding(nn.Module):\n",
    "    def __init__(self,d_model,vocab):\n",
    "        super(Embeding,self).__init__()\n",
    "        self.emb = nn.Embedding(vocab,d_model)\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.emb(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEncoding(nn.Module):\n",
    "    def __init__(self,d_model,dropout,max_len = 500):\n",
    "        super(PositionEncoding,self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "        positionEncode = torch.zeros(max_len,d_model)\n",
    "        position = torch.arange(0,max_len).unsqueeze(1) # max_len,1\n",
    "        div_term = torch.exp(torch.arange(0,d_model,2) * -(math.log(10000.0)/d_model))\n",
    "        \n",
    "        positionEncode[:,0::2] = torch.sin(position*div_term) # even\n",
    "        positionEncode[:,1::2] = torch.cos(position*div_term) # odd\n",
    "        positionEncode = positionEncode.unsequeeze(0) # add dimension in forst axis\n",
    "        self.register_buffer('pe',positionEncode)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x +Variable(self.positionEncode[:,:x.size(1)],\n",
    "                       require_grad = False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 25])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(30,50)[:,0::2].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 4, 6, 8])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0,10)[0::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
