{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os \n",
    "import re\n",
    "\n",
    "html = urlopen('https://storm.cis.fordham.edu/~yli/data/MyShakespeare.txt').read().decode('utf-8','ignore')\n",
    "soup = BeautifulSoup(html, features='lxml')\n",
    "all_href = soup.find_all('p')\n",
    "l = re.sub(r'\\r\\n\\r\\n','[P]',str(all_href))\n",
    "l = re.sub(r'<.*?>','',l)\n",
    "l = re.sub(r'\\r\\n',' ',l)\n",
    "l = re.sub(r'\\[P\\]','\\r\\n\\r\\n',l)\n",
    "\n",
    "with open('data.txt','w')as f:\n",
    "    f.write(l[1:-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_WORDS = {'PADDING': '<PAD>'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "class NgramModel(object):\n",
    "    def __init__(self,n = 3):\n",
    "        super(NgramModel,self).__init__()\n",
    "        self.n = n # Tell me how many gram you want?\n",
    "        self.create_lookup_tables = self.create_lookup_tables\n",
    "        self.token_lookup = self._token_lookup()\n",
    "        self.vocab_to_int = None\n",
    "        self.int_to_vocab = None\n",
    "        self.word_counter = None\n",
    "        self.int_text = None\n",
    "        self.corpus = None\n",
    "        self.ngram_matrix = None\n",
    "        self.gram_counter = None\n",
    "        self.ngram_1matrix = None\n",
    "        self.pp = None\n",
    "    \n",
    "        assert self.n > 1, \"N should larger than 1 !!!!!\"\n",
    "        \n",
    "    \n",
    "    def load_data(self,path):\n",
    "        input_file = os.path.join(path)\n",
    "        with open(input_file, \"r\") as f:\n",
    "            data = f.read()\n",
    "        return data    \n",
    "    \n",
    "    def _token_lookup(self):\n",
    "        answer = {'.' : '||period||',\n",
    "                  ',' : '||comma||',\n",
    "                  '\"' : '||quotation_mark||',\n",
    "                  ';' : '||semicolon||',\n",
    "                  '!' : '||exclamation_mark||',\n",
    "                  '?' : '||question_mark||',\n",
    "                  '(' : '||left_Parentheses||',\n",
    "                  ')' : '||right_Parentheses||',\n",
    "                  #'\\n': '||return||',\n",
    "                  '-' : '||dash||'}\n",
    "        return answer\n",
    "    \n",
    "    def update(self,text):\n",
    "        text = self.load_data(text)\n",
    "        text = self.preprocessing(text).lower()\n",
    "        self.corpus = ['<START> '* (self.n-1) + t + ' <END>' * (self.n-1) for t in text.split('\\n\\n')]\n",
    "        \n",
    "        text = text.split()\n",
    "        self.word_counter = Counter(text)\n",
    "        self.vocab_to_int, self.int_to_vocab = self.create_lookup_tables(text + list(SPECIAL_WORDS.values()))\n",
    "        self.int_text = [self.vocab_to_int[word] for word in text]\n",
    "        \n",
    "        \n",
    "    def preprocessing(self,text):\n",
    "        for key, token in self.token_lookup.items():\n",
    "            text = text.replace(key, ' {} '.format(token))\n",
    "        return text\n",
    "    \n",
    "    def create_lookup_tables(self,text):\n",
    "        vocab_to_int = { v:i+2 for i,v in enumerate(set(text))}\n",
    "        vocab_to_int['<START>'] = 0\n",
    "        vocab_to_int['<end>'] = 1\n",
    "        int_to_vocab = { v:k for k,v in vocab_to_int.items()}\n",
    "        # return tuple\n",
    "        return (vocab_to_int, int_to_vocab)\n",
    "        \n",
    "    \n",
    "    def get_vocab(self):\n",
    "        return self.vocab_to_int\n",
    "    \n",
    "    def size_vocab(self):\n",
    "        return len(self.vocab_to_int)\n",
    "    \n",
    "    def get_gram(self):\n",
    "        self._n_1gram()\n",
    "        m = []\n",
    "        for i in self.corpus:\n",
    "            try:\n",
    "                if len(i.split()) < self.n:\n",
    "                    ng = self.pad(i.split)\n",
    "                    m.append(tuple(ng))\n",
    "                else:\n",
    "                    for j in range(len(i.split())-self.n):\n",
    "                        ng = i.split()[j:j+self.n]\n",
    "                        m.append(tuple(ng))\n",
    "            except:\n",
    "                KeyboardInterrupt\n",
    "        self.ngram_matrix = m\n",
    "    \n",
    "        \n",
    "    def _n_1gram(self):\n",
    "        m = []\n",
    "        for i in self.corpus:\n",
    "            try:\n",
    "                if len(i.split()) < self.n:\n",
    "                    ng = self.pad(i.split)\n",
    "                    m.append(tuple(ng))\n",
    "                else:\n",
    "                    for j in range(len(i.split())-self.n+1):\n",
    "                        ng = i.split()[j:j+self.n-1]\n",
    "                        m.append(tuple(ng))\n",
    "            except:\n",
    "                KeyboardInterrupt\n",
    "        self.ngram_1matrix = m\n",
    "        \n",
    "    \n",
    "    def len_text(self):\n",
    "        return len(self.text.split())-2\n",
    "    \n",
    "    def len_ngram(self):\n",
    "        return len(self.ngram_matrix)\n",
    "    \n",
    "    def word_freq(self,word):\n",
    "        print(self.counter[word])\n",
    "            \n",
    "        \n",
    "    def pad(self,text,):\n",
    "        l = len(text)\n",
    "        n = self.n-l\n",
    "        for _ in range(n):\n",
    "            text.append('<PAD>')\n",
    "        return text\n",
    "    \n",
    "    def len_gram(self):\n",
    "        return len(self.ngram_matrix)\n",
    "    \n",
    "    def ngram_freq(self,gram):\n",
    "        gram = self.preprocessing(gram)\n",
    "        test = [ i for i in gram.lower().split()]\n",
    "        assert len(test) == self.n, 'It seems the length of you input is not match !!'\n",
    "        try:\n",
    "            if self.gram_counter == None:\n",
    "                self.gram_counter = Counter(self.ngram_matrix)\n",
    "            \n",
    "            if self.gram_counter[tuple(test)] == 0 :\n",
    "                print('Come on, we dont have these combo !!')\n",
    "                pro = 1/(self.size_vocab()*2)\n",
    "                print('Probobility is {a}'.format(a=pro))\n",
    "            else:                     \n",
    "                #print(self.gram_counter[tuple(test)])\n",
    "                return self.gram_counter[tuple(test)]\n",
    "        except:\n",
    "            KeyboardInterrupt\n",
    "            \n",
    "    def n_1gram_freq(self,gram):\n",
    "        gram = self.preprocessing(gram)\n",
    "        test = [ i for i in gram.lower().split()]\n",
    "        test.pop()\n",
    "        try:\n",
    "            gram_counter = Counter(self.ngram_1matrix)\n",
    "            \n",
    "            if gram_counter[tuple(test)] == 0 :\n",
    "                print('Come on, we dont have these combo !!')\n",
    "                return \n",
    "            else:                     \n",
    "                #print(self.gram_counter[tuple(test)])\n",
    "                return gram_counter[tuple(test)]\n",
    "        except:\n",
    "            KeyboardInterrupt\n",
    "            \n",
    "    def get_pro(self,ngram):\n",
    "        if self.ngram_freq(ngram):\n",
    "            return self.ngram_freq(ngram)/self.n_1gram_freq(ngram)\n",
    "        else:\n",
    "            return 1/(len(self.vocab_to_int)+1)\n",
    "        \n",
    "                \n",
    "                \n",
    "    def text_generate(self,gram, min_length, max_length):\n",
    "        gram = self.preprocessing(gram)\n",
    "        test = [ i for i in gram.lower().split()]\n",
    "        print(test)\n",
    "        \n",
    "        \n",
    "        assert len(test) >= self.n -1, 'You are too short to gen !!!!'\n",
    "        while len(test) <= max_length:\n",
    "            \n",
    "            gen = tuple(test[-(self.n-1):])\n",
    "            #print(gen)\n",
    "            test.append(self.findCondition(gen))\n",
    "            #print(test)\n",
    "            if self.findCondition(gen) == '<END>':\n",
    "                return self.parse(test)\n",
    "        return self.parse(test)\n",
    "        \n",
    "    def findCondition(self,n_1gram):\n",
    "        candidate = [i for i in self.ngram_matrix if i[:self.n-1] == n_1gram]\n",
    "        #print(candidate)\n",
    "        c = Counter(candidate)\n",
    "        #print(len(c.most_common(1)))\n",
    "        try:\n",
    "            if len(c.most_common(1))>0:\n",
    "                next_word = c.most_common(1)[0][0][-1]\n",
    "                return next_word\n",
    "            return\n",
    "        except:\n",
    "            KeyboardInterrupt\n",
    "        \n",
    "    def parse(self,text):\n",
    "        #print(text)\n",
    "        if text[-1] == '<END>':\n",
    "            text = \" \".join(text)\n",
    "            for token, key in self.token_lookup.items():\n",
    "                text = text.replace(key, ' {} '.format(token))\n",
    "                #print(text)\n",
    "            return \" \".join(re.findall(r'.+(?=\\s\\<END\\>)',text)[0].split()) \n",
    "        \n",
    "        else:\n",
    "            text = \" \".join(text)\n",
    "            for token, key in self.token_lookup.items():\n",
    "                text = text.replace(key, ' {} '.format(token))\n",
    "            return text\n",
    "        \n",
    "    def perplexity(self,text):\n",
    "        gram = '<START> '* (self.n - 1) + self.preprocessing(text)\n",
    "        test = [ i for i in gram.lower().split()]\n",
    "        #print(len(test))\n",
    "        pp = 1\n",
    "        import math\n",
    "        for i in range(len(test)-self.n+1):\n",
    "            ngram = \" \".join(test[i:i+self.n])\n",
    "            #print(1/self.get_pro(ngram))\n",
    "            pp *= 1/self.get_pro(ngram)\n",
    "        print(pp**(1/((len(test)-2))))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng = NgramModel(2)\n",
    "ng.update('data.txt')\n",
    "ng.get_gram()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "It seems the length of you input is not match !!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-200-29efeac7e51c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngram_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'members, the'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'members, the'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-198-7674ff55ae9f>\u001b[0m in \u001b[0;36mngram_freq\u001b[0;34m(self, gram)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mgram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'It seems the length of you input is not match !!'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgram_counter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: It seems the length of you input is not match !!"
     ]
    }
   ],
   "source": [
    "ng.ngram_freq('members, the'),ng.get_pro('members, the')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['our', 'business']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'our business is  ,  and hear me  ,  and hear me  ,  and hear me  ,  and hear me  ,  and hear me  ,  and hear me  ,  and hear me'"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng.text_generate('our business',10,30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Come on, we dont have these combo !!\n",
      "Probobility is 0.0003244646333549643\n",
      "Come on, we dont have these combo !!\n",
      "Probobility is 0.0003244646333549643\n",
      "Come on, we dont have these combo !!\n",
      "Probobility is 0.0003244646333549643\n",
      "774.6404268972499\n"
     ]
    }
   ],
   "source": [
    "ng.perplexity('make you a sword for me')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1541"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ng.vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '9', '2', '7', '0', '7', '4', '0', '9', '8', '8', '1', '8', '3', '0', '2', '0', '8', '0', '0', '6', '0', '5', '3', '5', '5', '3', '8', '6', '8', '3', '1', '4', '1', '5', '4', '0', '2', '6', '4', '8', '1', '0', '2', '4', '8', '2', '1', '2', '6', '0', '0', '4', '4', '4', '2', '7', '0', '7', '9', '2', '4', '0', '4', '7', '1', '9', '6', '3', '2', '8', '6', '9', '9', '6', '0', '2', '7', '7', '8', '3', '7', '9', '4', '9', '7', '0', '0', '7', '3', '3', '6', '9', '3', '6', '0', '0', '8', '1', '4', '7', '5', '1', '9', '5', '7', '6', '2', '6', '3', '0', '3', '9', '7', '6', '1', '0', '7', '3', '1', '3', '1', '4', '2', '0', '4', '4', '5', '8', '3', '9', '5', '9', '3', '7', '5', '7', '9', '8', '1', '4', '7', '7', '8', '2', '3', '2', '8', '0', '6', '2', '9', '8', '5', '0', '8', '5', '7', '9', '6', '6', '8', '7', '7', '9', '9', '3', '5', '7', '2', '1', '1', '2', '6', '4', '2', '6', '8', '4', '7', '5', '1', '6', '6', '2', '3', '3', '0', '9', '4', '3', '1', '3', '9', '3', '9', '4', '0', '7', '9', '9', '9', '3', '8', '2', '6', '9', '8', '0', '3', '8', '9', '9', '3', '2', '4', '0', '0', '7', '2', '4', '8', '0', '8', '2', '9', '1', '5', '8', '9', '2', '3', '9', '6', '6', '3', '5', '2', '2', '0', '3', '8', '2', '0', '4', '2', '1', '6', '1', '7', '1', '9', '0', '3', '6', '7', '6', '7', '5', '6', '7', '1', '3', '6', '2', '1', '1', '3', '5', '8', '2', '6', '6', '4', '4', '4', '1', '1', '4', '9', '7', '0', '5', '2', '7', '7', '1', '1', '8', '4', '4', '1', '4', '9', '6', '3', '8', '6', '0', '2', '8', '2', '7', '6', '8', '7', '0', '3', '5', '1', '8', '3', '7', '4', '4', '5', '4', '8', '0', '4', '6', '9', '2', '6', '3', '6', '0', '0', '6', '2', '5', '8', '5', '5', '6', '3', '2', '9', '2', '3', '3', '6', '5', '2', '3', '4', '2', '5', '9', '5', '2', '6', '8', '2', '4', '2', '4', '7', '8', '6', '2', '0', '8', '2', '3', '3', '2', '8', '3', '3', '2', '0', '1', '7', '9', '1', '7', '5', '8', '0', '1', '9', '7', '2', '3', '6', '2', '4', '5', '0', '2', '6', '7', '4', '7', '0', '6', '8', '2', '9', '8', '3', '3', '2', '4', '5', '4', '3', '9', '9', '2', '5', '2', '7', '3', '8', '8', '8', '1', '8', '0', '2', '1', '4', '9', '4', '0', '6', '0', '5', '5', '7', '8', '6', '0', '8', '6', '4', '4', '5', '0', '3', '1', '6', '7', '4', '4', '3', '8', '0', '1', '8', '7', '4', '4', '8', '5', '3', '0', '9', '2', '0', '8', '4', '4', '0', '5', '0', '2', '8', '2', '6', '6', '2', '3', '2', '0', '7', '2', '4', '5', '8', '6', '2', '8', '4', '2', '5', '0', '1', '8', '4', '3', '2', '7', '0', '0', '0', '8', '4', '7', '0', '6', '4', '3', '7', '2', '1', '6', '7', '7', '1', '3', '3', '8', '9', '5', '8', '0', '0', '6', '2', '5', '0', '2', '9', '4', '6', '4', '7', '4', '5', '9', '0', '7', '2', '0', '1', '7', '2', '7', '3', '8', '8', '8', '8', '5', '4', '8', '4', '8', '8', '5', '4', '7', '4', '9', '7', '0', '5', '2', '5', '9', '6', '4', '6', '5', '2', '8', '6', '5', '3', '8', '0', '5', '5', '1', '6', '7', '7', '0', '9', '0', '2', '1', '8', '0', '0', '8', '9', '2', '4', '5', '7', '1', '5', '0', '5', '1', '0', '5', '2', '0', '1', '6', '0', '3', '6', '8', '1', '1', '7', '5', '0', '3', '5', '7', '2', '9', '0', '2', '3', '3', '6', '9', '2', '5', '3', '2', '3', '0', '9', '4', '4', '3', '7', '6', '0', '4', '9', '9', '2', '9', '0', '1', '9', '6', '5', '7', '9', '5', '0', '0', '4', '0', '4', '7', '5', '5', '4', '6', '3', '3', '5', '8', '7', '6', '8', '4', '6', '9', '4', '1', '8', '1', '3', '9', '9', '1', '6', '2', '4', '7', '8', '5', '3', '2', '9', '2', '8', '3', '6', '0', '9', '5', '1', '7', '4', '2', '4', '8', '0', '3', '9', '8', '4', '5', '1', '4', '8', '9', '6', '4', '6', '0', '8', '8', '6', '8', '5', '6', '4', '5', '7', '8', '9', '4', '2', '6', '8', '8', '7', '9', '4', '6', '6', '3', '4', '9', '3', '4', '6', '7', '6', '8', '4', '9', '2', '8', '4', '6', '0', '8', '7', '4', '2', '6', '4', '4', '7', '7', '5', '7', '5', '6', '6', '7', '5', '0', '8', '9', '1', '1', '5', '5', '7', '8', '0', '6', '8', '2', '4', '9', '7', '3', '9', '2', '3', '1', '0', '6', '3', '4', '5', '8', '9', '3', '1', '1', '2', '9', '1', '2', '0', '8', '0', '3', '7', '5', '1', '2', '1', '4', '3', '7', '9', '0', '5', '7', '4', '3', '5', '4', '2', '1', '0', '1', '6', '0', '6', '4', '5', '9', '8', '2', '5', '7', '7', '5', '7', '7', '0', '8', '6', '4', '1', '2', '8', '6', '8', '7', '8', '2', '7', '5', '2', '3', '2', '5', '6', '0', '7', '2', '5', '7', '6', '4', '1', '9', '9', '2', '8', '8', '1', '7', '0', '1', '0', '0', '3', '9', '1', '7', '2', '6', '3', '8', '4', '4', '0', '9', '3', '8', '2', '7', '1', '3', '0', '9', '2', '8', '9', '1', '9', '9', '3', '3', '2', '1', '7', '8', '2', '5', '2', '0', '3', '4', '1', '9', '9', '6', '7', '5', '5', '8', '5', '0', '3', '9', '5', '5', '0', '7', '3', '0', '2', '5', '6', '7', '1', '1', '5', '4', '7', '3', '4', '9', '5', '6', '9', '5', '8', '7', '8', '0', '2', '1', '4', '7', '6', '2', '2', '4', '2', '6', '6', '4', '6', '0', '8', '6', '3', '1', '1', '9', '2', '6', '4', '7', '8', '4', '6', '1', '0', '7', '3', '5', '9', '9', '0', '1', '2', '6', '9', '4', '4', '9', '1', '3']\n"
     ]
    }
   ],
   "source": [
    "# 随机生成 vocab 为10 的 sequence\n",
    "a = [str(np.random.randint(0,10)) for _ in range(1000)]\n",
    "print(a)\n",
    "\n",
    "# 2, 3, 比较 \n",
    "tokens2 = (\"<START>\\n\" * 1 + \"\\n\".join(a)).split('\\n')\n",
    "tokens3 = (\"<START>\\n\" * 2 + \"\\n\".join(a)).split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8', '2', '1', '6', '3', '4']\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "def get_gram(token,n):\n",
    "    m = []\n",
    "    for i in range(len(token)-n+1):\n",
    "        ng = token[i:i+n]\n",
    "        m.append(tuple(ng))\n",
    "    return m\n",
    "\n",
    "def ngram_freq(test=('6','7'),counter=counter):\n",
    "\n",
    "    if counter[tuple(test)] ==0:\n",
    "        return 0\n",
    "    else:                     \n",
    "        return counter[tuple(test)]\n",
    "    \n",
    "def pro(gram,counter,jianyicounter):\n",
    "    leng = len(gram)-1\n",
    "    num = gram[:leng]\n",
    "    #print(num)\n",
    "    if ngram_freq(gram,counter=counter)!= 0 and ngram_freq(num,counter=jianyicounter)!=0:\n",
    "        return ngram_freq(gram,counter=counter)/ngram_freq(num,counter=jianyicounter)\n",
    "    else:\n",
    "        return 0.1\n",
    "\n",
    "        \n",
    "gram2 = get_gram(tokens2,2)\n",
    "counter2 =Counter(gram)\n",
    "gram3 = get_gram(tokens3,3)\n",
    "counter3 =Counter(gram)\n",
    "counter1 =Counter([tuple(i) for i in a])\n",
    "test = [str(np.random.randint(0,10)) for _ in range(6)]\n",
    "test3 = (\"<START>\\n\" * N + \"\\n\".join(test)).split('\\n')\n",
    "test2 = (\"<START>\\n\" * 1 + \"\\n\".join(test)).split('\\n')\n",
    "print(test)\n",
    "test3gram = get_gram(test3,3)\n",
    "test2gram = get_gram(test2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<START>', '<START>', '8'), ('<START>', '8', '2'), ('8', '2', '1'), ('2', '1', '6'), ('1', '6', '3'), ('6', '3', '4')]\n",
      "\n",
      "[('<START>', '8'), ('8', '2'), ('2', '1'), ('1', '6'), ('6', '3'), ('3', '4')]\n"
     ]
    }
   ],
   "source": [
    "print(test3gram)\n",
    "print()\n",
    "\n",
    "print(test2gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000.0"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3gram 结果 \n",
    "a =1\n",
    "for i in test3gram:\n",
    "    a *=(1/pro(i,counter3,counter2))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "620804.029090909"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 gram 结果 \n",
    "b=1\n",
    "for i in test2gram:\n",
    "    b *=(1/pro(i,counter2,counter1))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
