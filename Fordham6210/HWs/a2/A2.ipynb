{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "html = urlopen('https://storm.cis.fordham.edu/~yli/data/MyShakespeare.txt').read().decode('utf-8','ignore')\n",
    "soup = BeautifulSoup(html, features='lxml')\n",
    "\n",
    "all_href = soup.find_all('p')\n",
    "\n",
    "\n",
    "l = re.sub(r'\\r\\n\\r\\n','[P]',str(all_href))\n",
    "l = re.sub(r'<.*?>','',l)\n",
    "l = re.sub(r'\\r\\n',' ',l)\n",
    "l = re.sub(r'\\[P\\]','\\r\\n\\r\\n',l)\n",
    "\n",
    "with open('data.txt','w')as f:\n",
    "    f.write(l[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_WORDS = {'PADDING': '<PAD>'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "class NgramModel(object):\n",
    "    def __init__(self,n = 3):\n",
    "        super(NgramModel,self).__init__()\n",
    "        self.n = n\n",
    "        self.create_lookup_tables = self.create_lookup_tables\n",
    "        self.token_lookup = self._token_lookup()\n",
    "        self.vocab_to_int = None\n",
    "        self.int_to_vocab = None\n",
    "        self.word_counter = None\n",
    "        self.int_text = None\n",
    "        self.corpus = None\n",
    "        self.ngram_matrix = None\n",
    "        self.gram_counter = None\n",
    "        assert self.n > 1, \"N should larger than 1 !!!!!\"\n",
    "        \n",
    "    \n",
    "    def load_data(self,path):\n",
    "        input_file = os.path.join(path)\n",
    "        with open(input_file, \"r\") as f:\n",
    "            data = f.read()\n",
    "        return data    \n",
    "    \n",
    "    def _token_lookup(self):\n",
    "        answer = {'.' : '||Period||',\n",
    "                  ',' : '||Comma||',\n",
    "                  '\"' : '||Quotation_Mark||',\n",
    "                  ';' : '||Semicolon||',\n",
    "                  '!' : '||Exclamation_mark||',\n",
    "                  '?' : '||Question_mark||',\n",
    "                  '(' : '||Left_Parentheses||',\n",
    "                  ')' : '||Right_Parentheses||',\n",
    "                  #'\\n': '||Return||',\n",
    "                  '-' : '||Dash||'}\n",
    "        return answer\n",
    "    \n",
    "    def update(self,text):\n",
    "        text = self.load_data(text)\n",
    "        text = self.preprocessing(text).lower()\n",
    "        self.corpus = ['<START> ' + t + ' <END>' for t in text.split('\\n\\n')]\n",
    "        \n",
    "        text = text.split()\n",
    "        self.word_counter = Counter(text)\n",
    "        self.vocab_to_int, self.int_to_vocab = self.create_lookup_tables(text + list(SPECIAL_WORDS.values()))\n",
    "        self.int_text = [self.vocab_to_int[word] for word in text]\n",
    "        \n",
    "        \n",
    "    def preprocessing(self,text):\n",
    "        for key, token in self.token_lookup.items():\n",
    "            text = text.replace(key, ' {} '.format(token))\n",
    "        return text\n",
    "    \n",
    "    def create_lookup_tables(self,text):\n",
    "        vocab_to_int = { v:i+2 for i,v in enumerate(set(text))}\n",
    "        vocab_to_int['<START>'] = 0\n",
    "        vocab_to_int['<end>'] = 1\n",
    "        int_to_vocab = { v:k for k,v in vocab_to_int.items()}\n",
    "        # return tuple\n",
    "        return (vocab_to_int, int_to_vocab)\n",
    "        \n",
    "    \n",
    "    def get_vocab(self):\n",
    "        return self.vocab_to_int\n",
    "    \n",
    "    def size_vocab(self):\n",
    "        return len(self.vocab_to_int)\n",
    "    \n",
    "    def get_gram(self):\n",
    "        \n",
    "        m = []\n",
    "        for i in self.corpus:\n",
    "            try:\n",
    "                if len(i.split()) < self.n:\n",
    "                    ng = self.pad(i.split)\n",
    "                    m.append(tuple(ng))\n",
    "                else:\n",
    "                    for j in range(len(i.split())-self.n):\n",
    "                        ng = i.split()[j:j+self.n]\n",
    "                        m.append(tuple(ng))\n",
    "            except:\n",
    "                KeyboardInterrupt\n",
    "        \n",
    "           \n",
    "        self.ngram_matrix = m\n",
    "    \n",
    "    def len_text(self):\n",
    "        return len(self.text.split())-2\n",
    "    \n",
    "    def len_ngram(self):\n",
    "        return len(self.ngram_matrix)\n",
    "    \n",
    "    def word_freq(self,word):\n",
    "        print(self.counter[word])\n",
    "            \n",
    "        \n",
    "    def pad(self,text,):\n",
    "        l = len(text)\n",
    "        n = self.n-l\n",
    "        for _ in range(n):\n",
    "            text.append('<PAD>')\n",
    "        return text\n",
    "    \n",
    "    def len_gram(self):\n",
    "        return len(self.ngram_matrix)\n",
    "    \n",
    "    def ngram_freq(self,gram):\n",
    "        gram = self.preprocessing(gram)\n",
    "        test = [ i for i in gram.lower().split()]\n",
    "        assert len(test) == self.n, 'It seems the length of you input is not match !!'\n",
    "        try:\n",
    "            if self.gram_counter == None:\n",
    "                self.gram_counter = Counter(self.ngram_matrix)\n",
    "            \n",
    "            if self.gram_counter[tuple(test)] == 0 :\n",
    "                print('Come on, we dont have these combo !!')\n",
    "                pro = 1/(self.size_vocab()*2)\n",
    "                print('Probobility is {a}'.format(a=pro))\n",
    "            else:                     \n",
    "                print(self.gram_counter[tuple(test)])\n",
    "        except:\n",
    "            KeyboardInterrupt\n",
    "                \n",
    "                \n",
    "    def text_generate(self,gram, min_length, max_length):\n",
    "        \n",
    "        gram = self.preprocessing(gram)\n",
    "        test = [ i for i in gram.lower().split()]\n",
    "        #print(test)\n",
    "        outcomt = [test]\n",
    "        assert len(test) >= self.n, 'You are too short to gen !!!!'\n",
    "        while len(outcomt[-1]) <= max_length:\n",
    "            gen = tuple(test[-2:])\n",
    "            #print(gen)\n",
    "            test.append(self.findCondition(gen))\n",
    "            outcomt.append(test)\n",
    "            \n",
    "        return outcomt[-1]\n",
    "        \n",
    "    def findCondition(self,n_1gram):\n",
    "        candidate = [i for i in self.ngram_matrix if i[:self.n-1] == n_1gram]\n",
    "        #print(candidate)\n",
    "        c = Counter(candidate)\n",
    "        #print(c.most_common(1))\n",
    "        next_word = c.most_common(1)[0][0][-1]\n",
    "        print(next_word)\n",
    "        return next_word\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng = NgramModel(3)\n",
    "ng.update('data.txt')\n",
    "ng.get_gram()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "volsces\n",
      "are\n",
      "in\n",
      "arms\n",
      "||period||\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-294e924b0167>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'members, the'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-0af0b200fc66>\u001b[0m in \u001b[0;36mtext_generate\u001b[0;34m(self, gram, min_length, max_length)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;31m#print(gen)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindCondition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0moutcomt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-0af0b200fc66>\u001b[0m in \u001b[0;36mfindCondition\u001b[0;34m(self, n_1gram)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m#print(c.most_common(1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mnext_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "ng.text_generate('members, the',10,20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
