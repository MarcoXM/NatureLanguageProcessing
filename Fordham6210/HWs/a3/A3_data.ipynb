{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os \n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def getCorpus(link,tag):\n",
    "    html = urlopen(link).read().decode('utf-8','ignore')\n",
    "    soup = BeautifulSoup(html, features='lxml')\n",
    "    all_href = soup.find_all(tag)\n",
    "    \n",
    "    corpus = []\n",
    "    for i in all_href:\n",
    "        text =  i.get_text()\n",
    "        cleaned = re.sub(r'\\n','',text)\n",
    "        corpus.append(cleaned)\n",
    "    \n",
    "    return corpus\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "positivecorpus = getCorpus('https://storm.cis.fordham.edu/~yli/data/electronics/positive.review','review_text')\n",
    "negativecorpus = getCorpus('https://storm.cis.fordham.edu/~yli/data/electronics/negative.review','review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_WORDS = {'PADDING': '<PAD>'}\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from collections import Counter\n",
    "import string\n",
    "import pickle\n",
    "ps = PorterStemmer() \n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self,corpus,tokenizer,stemmer,\n",
    "                 remove_stop = True, \n",
    "                 keep_pantuation = True):\n",
    "        super(Dataset,self)\n",
    "        self.token_lookup = self._lookup()\n",
    "        self.vocab2idx = None\n",
    "        self.idx2vocab = None\n",
    "        self.word_counter = None\n",
    "        self._flatten = None\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokens = None\n",
    "        self.int_text = None\n",
    "        self.stemmer = stemmer\n",
    "        self.keep_pantuation = keep_pantuation\n",
    "        self.remove_stop = remove_stop\n",
    "        self.X = None\n",
    "        self.corpus = self._update(corpus)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _lookup(self):\n",
    "        ''' lookup table to keep the puntuation !'''\n",
    "        answer = {'.' : '||period||',\n",
    "                  ',' : '||comma||',\n",
    "                  '\"' : '||quotation_mark||',\n",
    "                  ';' : '||semicolon||',\n",
    "                  '!' : '||exclamation_mark||',\n",
    "                  '?' : '||question_mark||',\n",
    "                  '(' : '||left_Parentheses||',\n",
    "                  ')' : '||right_Parentheses||',\n",
    "                  #'\\n': '||return||',\n",
    "                  '-' : '||dash||'}\n",
    "        return answer\n",
    "        \n",
    "    def _update(self,text):\n",
    "        #text = flatten(text)\n",
    "        #print(text)\n",
    "        if self.keep_pantuation:\n",
    "            text = [self.preprocessing(t) for t in text]\n",
    "        else:\n",
    "            text = [re.sub(r\"[{}]+\".format(string.punctuation),'',t)  for t in text]\n",
    "        #print(text)\n",
    "            \n",
    "        text = [self.stemmer.stem(t.lower()) for t in text]\n",
    "\n",
    "        tokens = [self.tokenizer(t) for t in text]\n",
    "        \n",
    "        \n",
    "        if self.remove_stop:\n",
    "            tokens = tokens = [self.remove(t) for t in tokens]\n",
    "        self.X = tokens\n",
    "        tokens = flatten(tokens)\n",
    "        #print(len(tokens))\n",
    "        self.tokens = tokens\n",
    "        #print(tokens)\n",
    "        self.word_counter = Counter(self.tokens)\n",
    "        self.vocab2idx, self.idx2vocab = self.create_lookup_tables(text + list(SPECIAL_WORDS.values()))\n",
    "        self.int_text = [self.vocab2idx[word] for word in text]\n",
    "        \n",
    "        return text\n",
    "        \n",
    "    def create_lookup_tables(self,text):\n",
    "        vocab_to_int = { v:i+2 for i,v in enumerate(set(text))}\n",
    "        vocab_to_int['<START>'] = 0\n",
    "        vocab_to_int['<end>'] = 1\n",
    "        int_to_vocab = { v:k for k,v in vocab_to_int.items()}\n",
    "        # return tuple\n",
    "        return (vocab_to_int, int_to_vocab)\n",
    "            \n",
    "            \n",
    "    def remove(self,text):\n",
    "        from nltk.corpus import stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_words = [word for word in text if word not in stopwords.words('english')]\n",
    "        \n",
    "        return filtered_words\n",
    "        \n",
    "            \n",
    "    def preprocessing(self,text):\n",
    "        for key, token in self.token_lookup.items():\n",
    "            text = text.replace(key, ' {} '.format(token))\n",
    "        return text\n",
    "        \n",
    "        \n",
    "    def getmostwords(self,k):\n",
    "        \n",
    "        return sorted(self.word_counter.most_common(k), key=lambda x: x[1])\n",
    "    \n",
    "    def getToken(self):\n",
    "        return self.tokens\n",
    "    \n",
    "    def getTokenset(self):\n",
    "        return set(self.tokens) \n",
    "        \n",
    "    def getdata(self,X,y):\n",
    "        df = pd.DataFrame()\n",
    "        df['review'] = X\n",
    "        df['sentiment'] = y\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Dataset(corpus=positivecorpus, tokenizer=word_tokenize, stemmer=ps,keep_pantuation= False)\n",
    "n = Dataset(corpus=negativecorpus, tokenizer=word_tokenize, stemmer=ps,keep_pantuation= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('works', 184),\n",
       " ('ipod', 185),\n",
       " ('ive', 193),\n",
       " ('used', 202),\n",
       " ('phone', 203),\n",
       " ('bought', 206),\n",
       " ('price', 217),\n",
       " ('dont', 224),\n",
       " ('product', 230),\n",
       " ('well', 236),\n",
       " ('also', 240),\n",
       " ('get', 256),\n",
       " ('would', 258),\n",
       " ('quality', 274),\n",
       " ('like', 284),\n",
       " ('sound', 355),\n",
       " ('good', 399),\n",
       " ('great', 402),\n",
       " ('one', 429),\n",
       " ('use', 434)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.getmostwords(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positive = p.getdata(p.X,y = 1)\n",
    "df_negative = n.getdata(p.X,y = 0)\n",
    "df = pd.concat([df_positive,df_negative],axis=0)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df['review'] = df['review'].apply(lambda x : \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>use pink circles buff cddvd leaves circular ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unit works great much faster transfers using u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>little bouncy running convenient product fits ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fast shipping happy garmin tech support goodth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>headset works great comfortable wish better so...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  use pink circles buff cddvd leaves circular ma...          1\n",
       "1  unit works great much faster transfers using u...          1\n",
       "2  little bouncy running convenient product fits ...          0\n",
       "3  fast shipping happy garmin tech support goodth...          1\n",
       "4  headset works great comfortable wish better so...          0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(df.review,df.sentiment,test_size = 0.3)\n",
    "vectorizer = CountVectorizer()\n",
    "v_x_train = vectorizer.fit_transform(x_train)\n",
    "v_x_test = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from datetime import datetime\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class Models(object):\n",
    "    def __init__(self,X,y,kfolds):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.kfolds = kfolds\n",
    "        \n",
    "    def nb(self):\n",
    "        nb = MultinomialNB()\n",
    "        nb.fit(self.X,self.y)\n",
    "        return nb\n",
    "        \n",
    "        \n",
    "    def logistic(self):\n",
    "        lr = make_pipeline(LogisticRegressionCV(cv=self.kfolds))\n",
    "        lr.fit(self.X,self.y)\n",
    "\n",
    "        return lr\n",
    "    \n",
    "    def main(self):\n",
    "        print('START Fit')\n",
    "\n",
    "        print(datetime.now(), 'NB')\n",
    "        nb = self.nb()\n",
    "        print('Training done !!')\n",
    "\n",
    "        print(datetime.now(), 'logistics')\n",
    "        logistic = self.logistic()\n",
    "        print('Training done !!')\n",
    "        \n",
    "        return logistic,nb\n",
    "        \n",
    "    def save_model(self, model, path):\n",
    "        with open(path, 'wb') as clf:\n",
    "            pickle.dump(model, clf) \n",
    "              \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START Fit\n",
      "2019-09-28 19:39:27.997897 NB\n",
      "Training done !!\n",
      "2019-09-28 19:39:28.003397 logistics\n",
      "Training done !!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "models = Models(v_x_train,y_train,kfolds)\n",
    "log,nb = models.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
